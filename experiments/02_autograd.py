from turtle import forward
import torch
from torch.func import grad
import torch.nn as nn


print("=" * 60)
print("PyTorch è‡ªåŠ¨å¾®åˆ†æœºåˆ¶å®éªŒ")
print("=" * 60)

# ========== å®éªŒ 1: ç®€å•æ¢¯åº¦è®¡ç®— ==========
print("\nã€å®éªŒ 1ã€‘ç®€å•æ¢¯åº¦è®¡ç®—")
print("-" * 40)

x = torch.tensor([2.0, 3.0], requires_grad=True)
print(f"è¾“å…¥ x: {x}")

y = x ** 2
print(f"y = x^2: {y}")

loss = y.sum()
print(f"loss = sum(y): {loss.item()}")

# åå‘ä¼ æ’­
loss.backward()
print(f"æ¢¯åº¦ dx/dloss: {x.grad}")
print(f"ç†è®ºå€¼: [2*2, 2*3] = [4.0, 6.0]")

# ========== å®éªŒ 2: å¤šå±‚ç¥ç»ç½‘ç»œ ==========
print("\nã€å®éªŒ 2ã€‘ä¸¤å±‚ç¥ç»ç½‘ç»œè®­ç»ƒ")
print("-" * 40)

class TinyNet(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.fc1 = nn.Linear(10,20)
        self.fc2 = nn.Linear(20,1)

    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

device = "mps" if torch.backends.mps.is_available() else "cpu"
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

model = TinyNet().to(device)
print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters())}")

torch.manual_seed(42)
x = torch.randn(5,10, device=device)
y_true = torch.randn(5,1,device=device)


# è®­ç»ƒä¸€æ­¥
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
y_pred = model(x)
loss = ((y_pred - y_true)**2).mean()

print(f"\nè®­ç»ƒå‰æŸå¤±: {loss.item():.6f}")

loss.backward()
optimizer.step()

# å†æ¬¡å‰å‘ä¼ æ’­æŸ¥çœ‹æŸå¤±
with torch.no_grad():
    y_pred_new = model(x)
    loss_new = ((y_pred_new - y_true) ** 2).mean()

print(f"è®­ç»ƒåæŸå¤±: {loss_new.item():.6f}")
print(f"æŸå¤±ä¸‹é™: {loss.item() - loss_new.item():.6f}")

# ========== å®éªŒ 3: æ¢¯åº¦æµå¯è§†åŒ– ==========
print("\nã€å®éªŒ 3ã€‘æ¢¯åº¦æµåˆ†æ")
print("-" * 40)

# é‡æ–°åˆå§‹åŒ–æ¨¡å‹ä»¥æŸ¥çœ‹æ¢¯åº¦
model = TinyNet().to(device)
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

y_pred = model(x)
loss = ((y_pred - y_true) ** 2).mean()
loss.backward()


print("å„å±‚æ¢¯åº¦èŒƒæ•°:")
for name, param in model.named_parameters():
    if param.grad is not None:
        grad_norm = param.grad.norm().item()
        print(f"  {name:15s}: {grad_norm:.6f}")

# ========== å®éªŒ 4: æ¢¯åº¦ç´¯ç§¯æ¼”ç¤º ==========
print("\nã€å®éªŒ 4ã€‘æ¢¯åº¦ç´¯ç§¯ vs å¤§æ‰¹é‡")
print("-" * 40)


# æ–¹æ³• 1: å¤§æ‰¹é‡
model1 = TinyNet().to(device)
opt1 = torch.optim.SGD(model1.parameters(), lr=0.01)
x_large = torch.randn(8, 10, device=device)
y_large = torch.randn(8, 1, device=device)

opt1.zero_grad()
pred1 = model1(x_large)
loss1 = ((pred1 - y_large) ** 2).mean()
loss1.backward()

# ä¿å­˜æ¢¯åº¦
grad1 = model1.fc1.weight.grad.clone()

# æ–¹æ³• 2: æ¢¯åº¦ç´¯ç§¯ï¼ˆ2æ¬¡ batch_size=4ï¼‰
model2 = TinyNet().to(device)
model2.load_state_dict(model1.state_dict())  # ä½¿ç”¨ç›¸åŒåˆå§‹åŒ–
opt2 = torch.optim.SGD(model2.parameters(), lr=0.01)

opt2.zero_grad()
for i in range(2):
    x_micro = x_large[i*4:(i+1)*4]
    y_micro = y_large[i*4:(i+1)*4]
    pred2 = model2(x_micro)
    loss2 = ((pred2 - y_micro) ** 2).mean() / 2  # é™¤ä»¥ç´¯ç§¯æ¬¡æ•°
    loss2.backward()

grad2 = model2.fc1.weight.grad

# å¯¹æ¯”æ¢¯åº¦
grad_diff = (grad1 - grad2).abs().max().item()
print(f"å¤§æ‰¹é‡æ¢¯åº¦èŒƒæ•°: {grad1.norm().item():.6f}")
print(f"æ¢¯åº¦ç´¯ç§¯æ¢¯åº¦èŒƒæ•°: {grad2.norm().item():.6f}")
print(f"æœ€å¤§å·®å¼‚: {grad_diff:.8f}")
print(f"ç»“è®º: {'æ¢¯åº¦å‡ ä¹ç›¸åŒï¼' if grad_diff < 1e-5 else 'æœ‰å¾®å°æ•°å€¼è¯¯å·®'}")

print("\n" + "=" * 60)
print("âœ… æ‰€æœ‰å®éªŒå®Œæˆï¼")
print("=" * 60)

print("\nğŸ“š å…³é”®è¦ç‚¹:")
print("1. requires_grad=True ä½¿å¼ é‡å¯ä»¥è®¡ç®—æ¢¯åº¦")
print("2. .backward() è‡ªåŠ¨è®¡ç®—æ‰€æœ‰æ¢¯åº¦")
print("3. æ¢¯åº¦ç´¯ç§¯å¯ä»¥æ¨¡æ‹Ÿå¤§æ‰¹é‡è®­ç»ƒ")
print("4. MPS å¯¹ç¥ç»ç½‘ç»œè®­ç»ƒåŒæ ·æœ‰æ•ˆ")
